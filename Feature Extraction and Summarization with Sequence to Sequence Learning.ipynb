{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "czSR_WOeHqea"
   },
   "source": [
    "# KDD 2018 Hands-On Tutorial  https://kddseq2seq.com/\n",
    "\n",
    "Feature Extraction and Summarization With Sequence-to-Sequence Learning\n",
    "\n",
    "\n",
    "### Pre-requisites\n",
    "\n",
    "The target audience of this tutorial are moderately skilled users who have some familiarity with neural networks and are comfortable writing code.  These blog posts are good background for this tutorial:\n",
    "\n",
    "- [How To Create Data Products That Are Magical Using Sequence-to-Sequence Models](https://towardsdatascience.com/how-to-create-data-products-that-are-magical-using-sequence-to-sequence-models-703f86a231f8)\n",
    "\n",
    "- [How To Create Natural Language Semantic Search For Arbitrary Objects With Deep Learning](https://towardsdatascience.com/semantic-code-search-3cd6d244a39c)\n",
    "\n",
    "### Google Colab Notebooks\n",
    "\n",
    "This tutorial can be run in Google Colab notebooks, which provides a free gpu-enabled Jupyter Notebook on the cloud.  **You can open this notebook in Colab  by following [this link](https://colab.research.google.com/github/hohsiangwu/kdd-2018-hands-on-tutorials/blob/master/Feature%20Extraction%20and%20Summarization%20with%20Sequence%20to%20Sequence%20Learning.ipynb).**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "3ZqOslw71bkc"
   },
   "source": [
    "# Takeaway\n",
    "\n",
    "1. Language Model\n",
    "  * Self-supervised learning\n",
    "  * Sequence generation\n",
    "  * Pooling to get representations\n",
    "2. Sequence to Sequence Model\n",
    "  * Machine translation\n",
    "  * Encoder to get representations\n",
    "3. Joint Vector Space"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "-Tv_cXn6Gt0P"
   },
   "source": [
    "# Motivating Example: Semantic Code Search\n",
    "\n",
    "Yes, this is a gif of a notebook inside another notebook.\n",
    "\n",
    "Motivation:  What if you could search code semantically instead of keyword search?  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "KH8jBx7tGEQN"
   },
   "source": [
    "![alt text](https://github.com/hamelsmu/code_search/raw/master/gifs/live_search.gif?sanitize=true)\n",
    "\n",
    "A detailed, open source end to end tutorial on how to create semantic code search yourself is [here](https://towardsdatascience.com/semantic-code-search-3cd6d244a39c)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Q-gKT37Didb7"
   },
   "source": [
    "# Setup Notebook\n",
    "\n",
    "Install [ktext](https://github.com/hamelsmu/ktext) and [annoy](https://github.com/spotify/annoy)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "collapsed": true,
    "id": "E7l80u-0fyHK"
   },
   "outputs": [],
   "source": [
    "!pip install -q ktext\n",
    "!pip install -q annoy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "collapsed": true,
    "id": "W-KjInFk0v8l",
    "outputId": "45eaaa38-b15f-43d8-d676-6f35e2e5e94d"
   },
   "outputs": [],
   "source": [
    "import json\n",
    "from urllib.request import urlopen\n",
    "\n",
    "from annoy import AnnoyIndex\n",
    "from keras import optimizers\n",
    "from keras.layers import Input, Dense, LSTM, GRU, Embedding, Lambda, BatchNormalization\n",
    "from keras.models import Model\n",
    "from keras import optimizers\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.utils import to_categorical\n",
    "from ktext.preprocess import processor\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import random\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "iZzcs-PDPZqn"
   },
   "source": [
    "# Data sets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "McfeOLlh0v8x"
   },
   "source": [
    "## [CoNaLa](https://conala-corpus.github.io/)\n",
    "\n",
    "Challenge designed to test systems for generating program snippets from natural language.\n",
    "\n",
    "\n",
    "### Preview of the CoNaLa Dataset\n",
    "\n",
    "```\n",
    "{\n",
    "  \"question_id\": 36875258,\n",
    "  \"intent\": \"copying one file's contents to another in python\", \n",
    "  \"rewritten_intent\": \"copy the content of file 'file.txt' to file 'file2.txt'\", \n",
    "  \"snippet\": \"shutil.copy('file.txt', 'file2.txt')\", \n",
    "}\n",
    "\n",
    "{\n",
    "  \"intent\": \"How do I check if all elements in a list are the same?\", \n",
    "  \"rewritten_intent\": \"check if all elements in list `mylist` are the same\", \n",
    "  \"snippet\": \"len(set(mylist)) == 1\", \n",
    "  \"question_id\": 22240602\n",
    "}\n",
    "\n",
    "{\n",
    "  \"intent\": \"Iterate through words of a file in Python\", \n",
    "  \"rewritten_intent\": \"get a list of words `words` of a file 'myfile'\", \n",
    "  \"snippet\": \"words = open('myfile').read().split()\", \n",
    "  \"question_id\": 7745260\n",
    "}\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 289
    },
    "colab_type": "code",
    "collapsed": true,
    "id": "JAAOUd-k0v8x",
    "outputId": "3547d569-cf30-4a0a-8c85-bca60e0b66b6"
   },
   "outputs": [],
   "source": [
    "!wget http://www.phontron.com/download/conala-corpus-v1.1.zip\n",
    "!unzip -o conala-corpus-v1.1.zip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "collapsed": true,
    "id": "hFzeLKxg0v81"
   },
   "outputs": [],
   "source": [
    "with open('conala-corpus/conala-mined.jsonl', 'r') as f:\n",
    "    lines = [json.loads(line) for line in f.readlines()]\n",
    "source_docs = [line['snippet'] for line in lines]\n",
    "target_docs = [line['intent'] for line in lines]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 232
    },
    "colab_type": "code",
    "collapsed": true,
    "id": "QsXzuRH50v83",
    "outputId": "c4a5d6e2-26bb-4a10-88d7-8ef74d06250f"
   },
   "outputs": [],
   "source": [
    "with open('conala-corpus/conala-train.json', 'r') as f:\n",
    "    lines = json.load(f)\n",
    "train_source_docs = [line['snippet'] for line in lines]\n",
    "train_target_docs = [line['intent'] for line in lines]\n",
    "test_docs = [line['rewritten_intent'] for line in lines if line['rewritten_intent']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "collapsed": true,
    "id": "-t0x8A5w0v87"
   },
   "outputs": [],
   "source": [
    "with open('conala-corpus/conala-test.json', 'r') as f:\n",
    "    lines = json.load(f)\n",
    "test_source_docs = [line['snippet'] for line in lines]\n",
    "test_target_docs = [line['intent'] for line in lines]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "EhmB0qpr3RD5"
   },
   "source": [
    "## Other Data Sources (For Later Use)\n",
    "\n",
    "The below datasets are alternate sources of data for this same exercise.  We will not be reviewing these data as part of this tutorial.  However, we encourage you to inspect these data for additional practice and to get more intuition regarding these techniques.  Practicing with these other datasets will  give you confidence regarding the general application of the techniques we are teaching in this tutorial."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "oQLV9P8h0v8q"
   },
   "source": [
    "### [English to French](http://www.manythings.org/anki/)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "collapsed": true,
    "id": "1uK2D4090v8r"
   },
   "outputs": [],
   "source": [
    "# !wget http://www.manythings.org/anki/fra-eng.zip\n",
    "# !unzip -o fra-eng.zip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "collapsed": true,
    "id": "8UMp9B4M0v8u"
   },
   "outputs": [],
   "source": [
    "# with open('fra.txt', 'r') as f:\n",
    "#     lines = f.readlines()\n",
    "# target_docs, source_docs = zip(*[line.strip().split('\\t') for line in lines])\n",
    "# target_docs = list(set(target_docs))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "bw33N_AcPZqp"
   },
   "source": [
    "### GitHub issues data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "collapsed": true,
    "id": "GSR2uek3PZqq"
   },
   "outputs": [],
   "source": [
    "# issues = pd.read_csv('https://storage.googleapis.com/kubeflow-examples/github-issue-summarization-data/github-issues.zip')\n",
    "# source_docs = list(issues.body)\n",
    "# target_docs = list(issues.issue_title)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "AP58__OgPZqt"
   },
   "source": [
    "### Python (function, docstring) pairs\n",
    "\n",
    "Purpose of this dataset is to see if you can generate the docstring of a python function or method by looking at the code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "collapsed": true,
    "id": "izpU8EXGPZqu"
   },
   "outputs": [],
   "source": [
    "# f = urlopen('https://storage.googleapis.com/kubeflow-examples/code_search/data/train.function')\n",
    "# source_docs = [line.decode('utf-8') for line in f.readlines()]\n",
    "# f = urlopen('https://storage.googleapis.com/kubeflow-examples/code_search/data/train.docstring')\n",
    "# target_docs = [line.decode('utf-8') for line in f.readlines()]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "8JykNMln0v9I"
   },
   "source": [
    "## Use subset of the data\n",
    "\n",
    "We will use only of the training set in the interest of brevity.  However, we can use the full dataset in a subsequent pass if desired."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "collapsed": true,
    "id": "Hj1tDqDFPZqx"
   },
   "outputs": [],
   "source": [
    "source_docs = source_docs[:50000]\n",
    "target_docs = target_docs[:50000]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "lxYjchOjPZqz"
   },
   "source": [
    "# Language Model\n",
    "\n",
    "What is a language model?\n",
    "\n",
    "![alt text](https://cdn-images-1.medium.com/max/1440/1*XGfyUGtWq0yZ4RfufYfbRw.jpeg)\n",
    "\n",
    "Source: https://medium.com/paper-club/language-modeling-survey-333077e43dd9"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "COkelzEXPZq0"
   },
   "source": [
    "## Preprocessing\n",
    "Tokenize, generate vocabulary, apply padding and vectorize."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "PYXCJuN69MOo"
   },
   "source": [
    "#### Keras Text Pre-Processing Primer\n",
    "\n",
    "Now that we have gathered the data, we need to prepare the data for the modeling. Before jumping into the code, let’s warm up with a toy example of two documents:\n",
    "\n",
    "```\n",
    "[“The quick brown fox jumped over the lazy dog 42 times.”, “The dog is lazy”]\n",
    "```\n",
    "\n",
    "Below is a rough outline of the steps I will take in order to pre-processes this raw text:\n",
    "\n",
    "**1. Clean text:** in this step, we want to remove or replace specific characters and lower case all the text. This step is discretionary and depends on the size of the data and the specifics of your domain. In this toy example, I lower-case all characters and replace numbers with *number* in the text. In the real data, I handle more scenarios.\n",
    "\n",
    "[“the quick brown fox jumped over the lazy dog *number* times”, “the dog is lazy”]\n",
    "\n",
    "\n",
    "**3. Tokenize:** split each document into a list of words\n",
    "\n",
    "```\n",
    "[[‘the’, ‘quick’, ‘brown’, ‘fox’, ‘jumped’, ‘over’, ‘the’, ‘lazy’, ‘dog’, ‘*number*’, ‘times’], [‘the’, ‘dog’, ‘is’, ‘lazy’]]\n",
    "```\n",
    "\n",
    "**4. Build vocabulary:** You will need to represent each distinct word in your corpus as an integer, which means you will need to build a map of token -> integers. Furthermore, I find it useful to reserve an integer for rare words that occur below a certain threshold as well as 0 for padding (see next step). After you apply a token -> integer mapping, your data might look like this:\n",
    "\n",
    "```\n",
    "[[2, 3, 4, 5, 6, 7, 2, 8, 9, 10, 11], [2, 9, 12, 8]]\n",
    "```\n",
    "\n",
    "**5. Padding:** 5. Padding: You will have documents that have different lengths. There are many strategies on how to deal with this for deep learning, however for this tutorial I will pad and truncate documents such that they are all transformed to the same length for simplicity. You can decide to pad (with zeros) and truncate your document at the beginning or end, which I will refer to as “pre” and “post” respectively. After pre-padding our toy example, the data might look like this:\n",
    "\n",
    "```\n",
    "[[2, 3, 4, 5, 6, 7, 2, 8, 9, 10, 11], [0, 0, 0, 0, 0, 0, 0, 2, 9, 12, 8]]\n",
    "```\n",
    "\n",
    "A reasonable way to decide your target document length is to build a histogram of document lengths and choose a sensible number. (Note that the above example has padded the data in front but we could also pad at the end. We will discuss this more in the next section)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "6b7dZOxkDutv"
   },
   "source": [
    "Inspect the raw text of source and target documents:\n",
    "\n",
    "Source docs:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 187
    },
    "colab_type": "code",
    "collapsed": true,
    "id": "AdyYWgT_Eu13",
    "outputId": "81bf483b-b632-4f0c-e083-3896a3151088"
   },
   "outputs": [],
   "source": [
    "for x in source_docs[:10]:\n",
    "  print(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "hT8NcmEU4C4A"
   },
   "source": [
    "Target docs:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 187
    },
    "colab_type": "code",
    "collapsed": true,
    "id": "Ww4AHHUX4CPL",
    "outputId": "eb49cb52-a020-48aa-c3d2-29df310d9672"
   },
   "outputs": [],
   "source": [
    "target_docs[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "LTxQMjaa4WsL"
   },
   "source": [
    "In order to pre-process this data, we will use the [`ktext` package](https://github.com/hamelsmu/ktext).   `ktext` helps accomplish the pre-processing steps outlined in the previous section. This library is a thin wrapper around keras and spacy text processing utilities, and leverages python process-based-threading to speed things up. It also chains all of the pre-processing steps together and provides a bunch of convenience functions. Warning: this package is under development so use with caution outside this tutorial (pull requests are welcome!). To learn more about how this library works, look at this [tutorial](https://github.com/hamelsmu/ktext/blob/master/notebooks/Tutorial.ipynb) (but for now I suggest reading ahead)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 187
    },
    "colab_type": "code",
    "collapsed": true,
    "id": "uHOMxrONPZq3",
    "outputId": "563215cc-752b-47c0-eb2b-c19c7eefb7d6"
   },
   "outputs": [],
   "source": [
    "proc = processor(hueristic_pct_padding=.7, keep_n=5000)\n",
    "vecs = proc.fit_transform(target_docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "collapsed": true,
    "id": "5htxJQw81RLS"
   },
   "outputs": [],
   "source": [
    "assert vecs.shape[0] == len(target_docs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "0jld4HT0Dp_r"
   },
   "source": [
    "The above code cleans, tokenizes, and applies pre-padding and post-truncating such that each document length is equal to the 70th percentile of document lengths, which is an arbitrary choice. I made decisions about padding length by studying histograms of document length provided by ktext. Furthermore, only the top 5,000 words in the vocabulary are retained and remaining words are set to the index 1 which correspond to rare words (this was another arbitrary choice). \n",
    "\n",
    "Below is an example where tokens are mapped to integers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 51
    },
    "colab_type": "code",
    "collapsed": true,
    "id": "-ogmGKqUC-Jd",
    "outputId": "8f2015db-4a8e-450c-83d1-43809873ae36"
   },
   "outputs": [],
   "source": [
    "print('original list: ', target_docs[0])\n",
    "print('tokenized list: ', vecs[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ldna19QcEXYp"
   },
   "source": [
    "We can see the most common words here, by calling the `token_count_pandas()` method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 669
    },
    "colab_type": "code",
    "collapsed": true,
    "id": "dNOluhQpD8Fq",
    "outputId": "da73133f-b5a2-4084-f339-35af0ca8d5a7"
   },
   "outputs": [],
   "source": [
    "proc.token_count_pandas().head(20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Ab3JU2L5Ey9_"
   },
   "source": [
    "Furthermore, the documents in our corpus have different lengths. By setting `hueristic_pct_padding=.7`, `ktext` will truncate and pad all sequences to the 70th percentile length. However, it can be useful to sanity check a histogram of lengths. We inspect the `document_length_stats` property below which displays a histogram of document lengths."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 266
    },
    "colab_type": "code",
    "collapsed": true,
    "id": "ZIH4R1HREiTO",
    "outputId": "20fb81a8-389c-488d-b67c-aa305d9b9864"
   },
   "outputs": [],
   "source": [
    "proc.document_length_stats"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "SCNn24hRDxeF"
   },
   "source": [
    "It is useful to keep track of the maximum length and the unique number of tokens in the corpus for later purposes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 51
    },
    "colab_type": "code",
    "collapsed": true,
    "id": "wnjhEa-NPZq6",
    "outputId": "3c97f3d2-9548-4ebf-fd0d-02eed0defc02"
   },
   "outputs": [],
   "source": [
    "vocab_size = max(proc.id2token.keys()) + 1\n",
    "max_length = proc.padding_maxlen\n",
    "\n",
    "print('vocab size: ', vocab_size)\n",
    "print('max length allowed for documents: ', max_length)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "srQ2EApW0v9X"
   },
   "source": [
    "## Language model architecture"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "V31lnrq70v9T"
   },
   "source": [
    "Prepare training data for language model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "collapsed": true,
    "id": "Ioq7Sd9EPZq8",
    "outputId": "a883c721-a690-44c3-ea9d-3e428f3d8947"
   },
   "outputs": [],
   "source": [
    "sequences = []\n",
    "for arr in tqdm(vecs):\n",
    "    non_zero = (arr != 0).argmax()\n",
    "    for i in range(non_zero, len(arr)):\n",
    "        sequences.append(arr[:i+1])\n",
    "sequences = pad_sequences(sequences, maxlen=max_length, padding='pre')\n",
    "sequences = np.array(sequences)\n",
    "X, y = sequences[:,:-1], sequences[:,-1]\n",
    "# y = to_categorical(y, num_classes=vocab_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 306
    },
    "colab_type": "code",
    "collapsed": true,
    "id": "WKW7TCl6PZq_",
    "outputId": "506769ad-ba4c-42c2-9c84-9a138df5021d"
   },
   "outputs": [],
   "source": [
    "i = Input(shape=(max_length-1,))\n",
    "x = Embedding(vocab_size, 256, input_length=max_length-1)(i)\n",
    "x = LSTM(256, return_sequences=True)(x)\n",
    "last_timestep = Lambda(lambda x: x[:, -1, :])(x)\n",
    "last_timestep = Dense(vocab_size, activation='softmax')(last_timestep)\n",
    "model = Model(i, last_timestep)\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "jnVeYIZV0v9a"
   },
   "source": [
    "## Training\n",
    "\n",
    "Now that we have created our architecture, we can train our model.  \n",
    "\n",
    "**This step takes approximately 25 minutes.  This is a good time to take a bathroom break!**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 394
    },
    "colab_type": "code",
    "collapsed": true,
    "id": "Psve_dWVPZrB",
    "outputId": "8f522683-0a60-43ab-cc03-395953d9bded"
   },
   "outputs": [],
   "source": [
    "model.compile(loss='sparse_categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "history = model.fit(X, y, epochs=10, batch_size=128, validation_split=0.1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "cFH1DmGHPZrE"
   },
   "source": [
    "## Generate sequence\n",
    "\n",
    "The goal of a language model is to predict the next word in a sequence. To sanity check the language model, we will see what kind of sentence is generated when we start with a a seed word of 'is'. We are looking to see if the sentence generated appears to be sampled from the distribution of the data.\n",
    "\n",
    "In other words does the sentence generated look like it was written by the same author(s) pertaining to the same domain as the training corpus?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "collapsed": true,
    "id": "2R-2I_HRPZrE"
   },
   "outputs": [],
   "source": [
    "def generate_seq(model, proc, n_words, seed_text):\n",
    "    in_text = seed_text\n",
    "    for _ in range(n_words):\n",
    "        vec = proc.transform([in_text])[:,1:]\n",
    "        index = np.argmax(model.predict(vec, verbose=0), axis=1)[0]\n",
    "        out_word = ''\n",
    "        if index == 1:\n",
    "            out_word = '_unk_'\n",
    "        else:\n",
    "            out_word = proc.id2token[index]\n",
    "        in_text += ' ' + out_word\n",
    "    return in_text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "MCDhy30KqF2j"
   },
   "source": [
    "See what sentence is generated from language model, seeded witht he word `is`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "collapsed": true,
    "id": "8wpHSxIOPZrH",
    "outputId": "c5d225dc-d6d6-4847-f379-5a677e0c2c81"
   },
   "outputs": [],
   "source": [
    "generate_seq(model, proc, max_length, 'is')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "zzNZqTqVPZrJ"
   },
   "source": [
    "## Generate sentence embeddings\n",
    "\n",
    "One of the goals of training the language model is learning reprsentations of sentences in our corpus. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "aF90GHpSraaK"
   },
   "source": [
    "\n",
    "There are a plethora of general purpose pre-trained models that will generate high-quality embeddings of phrases (also called sentence embeddings). [This article](https://medium.com/huggingface/universal-word-sentence-embeddings-ce48ddc8fc3a) provides a great overview of the landscape. For example, Google’s universal sentence encoder works very well for many use cases and is available on [Tensorflow Hub](https://www.tensorflow.org/hub/modules/google/universal-sentence-encoder/1).\n",
    "\n",
    "Despite the convenience of these pre-trained models, it can be advantageous to train a model that captures the domain-specific vocabulary and semantics of docstrings. There are many techniques one can use to create sentence embeddings. These range from simple approaches, like averaging word vectors to more sophisticated techniques like those used in the construction of the universal sentence encoder.\n",
    "\n",
    "For this tutorial, we will leverage a the language model we trained earlier to generate embeddings for sentences.  It is important to carefully consider the corpus you use for training when building a language model. Ideally, you want to use a corpus that is of a similar domain to your downstream problem so you can adequately capture the relevant semantics and vocabulary. For example, a great corpus for this problem would be stack overflow data, since that is a forum that contains an extremely rich discussion of code. However, in order to keep this tutorial simple, we re-use the set of docstrings as our corpus. This is sub-optimal as discussions on stack overflow often contain richer semantic information than what is in a one-line docstring. We leave it as an exercise for the reader to examine the impact on the final outcome by using an alternate corpus.\n",
    "\n",
    "After we train the language model, our next task is to use this model to generate an embedding for each sentence. A common way of doing this is to summarize the hidden states of the language model.   A simple approach is to use aggregate stastics like the mean, max, or the sum of all the hidden states. There are other approaches that are outside the scope of this tutorial, and will discuss if time permits."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "fW1UeLKWqkMp"
   },
   "source": [
    "The below code extracts the hidden states from the encoder when given an input. There is one hidden state for each word in the sentence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "collapsed": true,
    "id": "Y-Rig9cePZrM"
   },
   "outputs": [],
   "source": [
    "embedding_model = Model(inputs=model.inputs, outputs=model.layers[-3].output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "i6aZsEL24Vdt"
   },
   "source": [
    "We can extract values from intermediate layers of this language model, and use those as sentence embeddings.  Here is how you can do that concretely with the language model we trained:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 306
    },
    "colab_type": "code",
    "collapsed": true,
    "id": "X-FFznLQPZrO",
    "outputId": "00e4aa9d-d7e4-4096-ffd0-6010e0ecff84"
   },
   "outputs": [],
   "source": [
    "input_sequence = test_docs[random.randint(0, len(test_docs))]\n",
    "print('input sequence: ', input_sequence, '\\n\\nhidden states:\\n')\n",
    "vec = proc.transform([input_sequence])[:,1:]\n",
    "embedding_model.predict(vec)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "PBksOOsJrTkk"
   },
   "source": [
    "Let's extract the hidden states for all the sentences in our training data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "collapsed": true,
    "id": "haQjC19y0v9l"
   },
   "outputs": [],
   "source": [
    "test_vecs = proc.transform(test_docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "collapsed": true,
    "id": "rLeLELte0v9n"
   },
   "outputs": [],
   "source": [
    "hidden_states = embedding_model.predict(test_vecs[:, 1:])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "9GjRzXnG4fT9"
   },
   "source": [
    "As mentioned earlier, we can compute aggregate statistics over the hidden states.  This is how you can do that:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "collapsed": true,
    "id": "SCf0kjpr0v9p"
   },
   "outputs": [],
   "source": [
    "mean_vecs = np.mean(hidden_states, axis=1)\n",
    "max_vecs = np.max(hidden_states, axis=1)\n",
    "sum_vecs = np.sum(hidden_states, axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "giC48Kt00v9s"
   },
   "source": [
    "## Application - Nearest Neighbor Search\n",
    "\n",
    "Now that we have a way to represent each sentence as a vector, we can use this representation on many kinds of downstream tasks. One such task is finding a similar sentence to any given sentence.  \n",
    "\n",
    "\n",
    "### Build vector indices\n",
    "\n",
    "We will first place all the vectorized sentences in a special data structure that allows for fast nearest neighbor lookups. We will use [annoy](https://github.com/spotify/annoy) for this purpose."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "collapsed": true,
    "id": "afBInH4C0v9s",
    "outputId": "960a9b9d-27f9-4046-9b11-7535bb7b9356"
   },
   "outputs": [],
   "source": [
    "dimension = hidden_states.shape[-1]\n",
    "index = AnnoyIndex(dimension)\n",
    "for i, v in enumerate(sum_vecs):\n",
    "    index.add_item(i, v)\n",
    "index.build(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "pp2KAWNj0v9u"
   },
   "source": [
    "### Search nearest neighbors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 241
    },
    "colab_type": "code",
    "collapsed": true,
    "id": "tr_tKEDx0v9w",
    "outputId": "564a1bc3-1833-412e-e96f-1649f2653f55"
   },
   "outputs": [],
   "source": [
    "input_sequence = test_docs[random.randint(0, len(test_docs))]\n",
    "print('Query: ', input_sequence)\n",
    "\n",
    "vec = proc.transform([input_sequence])[:,1:]\n",
    "vec = np.sum(embedding_model.predict(vec), axis=1)\n",
    "ids, _ = index.get_nns_by_vector(vec.T, 10, include_distances=True)\n",
    "\n",
    "print('\\nSearch Results:')\n",
    "[test_docs[i] for i in ids][1:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "pnPW20o9PZrR"
   },
   "source": [
    "# Sequence to Sequence Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "VpxbvU6OCDje"
   },
   "source": [
    "A [sequence to sequence model](https://towardsdatascience.com/how-to-create-data-products-that-are-magical-using-sequence-to-sequence-models-703f86a231f8) allows you to take an input sequence (source), and predict an output sequence (target).  These sequences can be anything, however we will focus on natural language for this tutorial. Sequence-to-sequence models have been used with great success in summarizing texts as well as generating translations from one language to another. For this tutorial, we will demonstrate a very creative task: given a snippet of code, we will train a model that generates a description of that code!\n",
    "\n",
    "### Sequence to Sequence Primer\n",
    "\n",
    "There are many variants of seq2seq models, however we will walk through one of the most simplest forms: an encoder-decoder network using RNNs.\n",
    "\n",
    "#### Training\n",
    "\n",
    "The decoder receives the ground truth, shifted by one time-step (is allowed to see the ground-truth of the previous time step).  This is called teacher forcing.   \n",
    "\n",
    "![alt text](https://blog.keras.io/img/seq2seq/seq2seq-teacher-forcing.png)\n",
    "\n",
    "#### Inference\n",
    "\n",
    "At inference time, we will not be able to see the ground truth from the last time step.   Therefore we can use the last predicted output in place of the previous time step's ground truth.  We will generate our sequence this way using a greedy approach, stopping only when we either reach a maximum length or predict a special <stop> token.  There are more sophisticated ways of generating sequences such as using [beam search](https://en.wikipedia.org/wiki/Beam_search) that we will not cover in this tutorial.\n",
    "\n",
    "![alt text](https://blog.keras.io/img/seq2seq/seq2seq-inference.png)\n",
    "\n",
    "Credit: https://blog.keras.io/category/tutorials.html\n",
    "\n",
    "\n",
    "**Building a neural network architecture is like stacking lego bricks.** For beginners, it can be useful to think of each layer as an API: you send the API some data and then the API returns some data. Thinking of things this way frees you from becoming overwhelmed, and you can build your understanding of things slowly. It is important to understand two concepts:\n",
    "\n",
    "the shape of data that each layer expects, and the shape of data the layer will return. (When you stack many layers on top of each other, the input and output shapes must be compatible, like legos).\n",
    "conceptually, what will the output(s) of a layer represent? What does the output of a subset of stacked layers represent?\n",
    "\n",
    "Let's take a look at the data we want to use. The `source` is the snippet of code and the `target` is the description of that code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 51
    },
    "colab_type": "code",
    "collapsed": true,
    "id": "ESQyR0l2CH01",
    "outputId": "657fcd12-6e53-44d1-e9bb-4b6c40316353"
   },
   "outputs": [],
   "source": [
    "print('source (code input): ', source_docs[2])\n",
    "print('target (description output): ', target_docs[2])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "BQsnI6720v9z"
   },
   "source": [
    "## Preprocessing\n",
    "\n",
    "Similar to previous excercises, we must pre-process the raw strings into a format that can be utilized by our model. One such format is to map each word in our corpus to a unique integer value, which we will refer to as a vocabulary. If the source and target are from the same distribution, (which they are not in this example) the vocabulary can be shared.\n",
    "\n",
    "\n",
    "Concretely, we will tokenize, generate vocabulary, apply padding and vectorize. These steps are as follows:\n",
    "\n",
    "**1. Tokenize:** Process of parsing strings into discrete words or tokens.\n",
    "\n",
    "**2. Generate Vocabulary:** Assign each token to a unique integer, rare-occuring tokens may be assigned to the same integer.\n",
    "\n",
    "**3. Padding:** We standardize the sequence length of each example to be the same by truncating and padding each example to the same lentgh.\n",
    "\n",
    "The `ktext` package helps us accomplish these steps."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 187
    },
    "colab_type": "code",
    "collapsed": true,
    "id": "s_nE-SeKPZrS",
    "outputId": "20a67276-b617-40ff-d7f5-5c63bacfc723"
   },
   "outputs": [],
   "source": [
    "source_proc = processor(hueristic_pct_padding=.7, keep_n=20000)\n",
    "source_vecs = source_proc.fit_transform(source_docs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "rdSNtX3MOjIX"
   },
   "source": [
    "Note that we will pre-process the source documents in the same way as the language model.  The target documents, however will be processed in the same way with some subtle differences. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 187
    },
    "colab_type": "code",
    "collapsed": true,
    "id": "ePq40vC1Oew0",
    "outputId": "ff376510-8b28-49af-93a8-35ddfd6eefe7"
   },
   "outputs": [],
   "source": [
    "target_proc = processor(append_indicators=True, hueristic_pct_padding=.7, keep_n=14000, padding ='post')\n",
    "target_vecs = target_proc.fit_transform(target_docs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "qkfDO83HE8Xc"
   },
   "source": [
    " Above, we passed some additional parameters:\n",
    "\n",
    " - **append_indicators=True** will append the tokens ‘_start_’ and ‘_end_’ to the start and end of each document, respectively.\n",
    " \n",
    " - **padding=’post’** means that zero padding will be added to the end of the document instead of default of ‘pre’.\n",
    " \n",
    " \n",
    " The reason for processing the target documents in this way is that we want our model to know when the first letter of the docstring is supposed to occur, and also learn to predict when the end of a phrase should be. This will make more sense in the next section where model architecture is discussed.\n",
    "\n",
    "Additionally, we will use teacher forcing for the decoder of the sequence to sequence model, so we will offset the target sequence by one."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "collapsed": true,
    "id": "3b0wFryNPZrU"
   },
   "outputs": [],
   "source": [
    "encoder_input_data = source_vecs\n",
    "encoder_seq_len = encoder_input_data.shape[1]\n",
    "\n",
    "decoder_input_data = target_vecs[:, :-1]\n",
    "decoder_target_data = target_vecs[:, 1:]\n",
    "\n",
    "num_encoder_tokens = max(source_proc.id2token.keys()) + 1\n",
    "num_decoder_tokens = max(target_proc.id2token.keys()) + 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "FCkBg1_1PZrZ"
   },
   "source": [
    "## Encoder model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "wvDkJOqoFHhf"
   },
   "source": [
    "The role of the encoder is to extract features and generate a representation of the input sequence, which in this case is a snippet of code. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "collapsed": true,
    "id": "nThkgdknPZra"
   },
   "outputs": [],
   "source": [
    "word_emb_dim=512\n",
    "hidden_state_dim=1024\n",
    "encoder_seq_len=encoder_seq_len\n",
    "num_encoder_tokens=num_encoder_tokens\n",
    "num_decoder_tokens=num_decoder_tokens\n",
    "\n",
    "encoder_inputs = Input(shape=(encoder_seq_len,), name='Encoder-Input')\n",
    "x = Embedding(num_encoder_tokens, word_emb_dim, name='Body-Word-Embedding', mask_zero=False)(encoder_inputs)\n",
    "x = BatchNormalization(name='Encoder-Batchnorm-1')(x)\n",
    "_, state_h = GRU(hidden_state_dim, return_state=True, name='Encoder-Last-GRU', dropout=.5)(x)\n",
    "encoder_model = Model(inputs=encoder_inputs, outputs=state_h, name='Encoder-Model')\n",
    "seq2seq_encoder_out = encoder_model(encoder_inputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 272
    },
    "colab_type": "code",
    "collapsed": true,
    "id": "LrcGequcQFCT",
    "outputId": "74660f07-3230-4103-ca1f-e77ecdc69ce4"
   },
   "outputs": [],
   "source": [
    "encoder_model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "8PnsgReVPZre"
   },
   "source": [
    "## Decoder model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "t9IbLqTCFQr3"
   },
   "source": [
    "The role of the decoder is to generate a description of the code conditioned on the features extracted by the encoder."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "collapsed": true,
    "id": "bu-KgmfjPZre"
   },
   "outputs": [],
   "source": [
    "decoder_inputs = Input(shape=(None,), name='Decoder-Input')\n",
    "dec_emb = Embedding(num_decoder_tokens, word_emb_dim, name='Decoder-Word-Embedding', mask_zero=False)(decoder_inputs)\n",
    "dec_bn = BatchNormalization(name='Decoder-Batchnorm-1')(dec_emb)\n",
    "decoder_gru = GRU(hidden_state_dim, return_state=True, return_sequences=True, name='Decoder-GRU', dropout=.5)\n",
    "decoder_gru_output, _ = decoder_gru(dec_bn, initial_state=seq2seq_encoder_out)\n",
    "x = BatchNormalization(name='Decoder-Batchnorm-2')(decoder_gru_output)\n",
    "decoder_dense = Dense(num_decoder_tokens, activation='softmax', name='Final-Output-Dense')\n",
    "decoder_outputs = decoder_dense(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "WfH-WvGwPZrg"
   },
   "source": [
    "## Sequence to sequence model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "kW3bg3XnFYqx"
   },
   "source": [
    "We can connect the encoder and decoder together to create the sequence to sequence model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "collapsed": true,
    "id": "xsNgKYlKPZri"
   },
   "outputs": [],
   "source": [
    "seq2seq_model = Model([encoder_inputs, decoder_inputs], decoder_outputs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "26w4XvF7S337"
   },
   "source": [
    "Summary of model architecture:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 425
    },
    "colab_type": "code",
    "collapsed": true,
    "id": "WZ1W2IrGSqZO",
    "outputId": "34b23c12-3143-4b4b-d5ac-69bedf02fb90"
   },
   "outputs": [],
   "source": [
    "seq2seq_model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "nvT-vl0TRMHI"
   },
   "source": [
    "![alt text](https://raw.githubusercontent.com/hohsiangwu/kdd-2018-hands-on-tutorials/master/images/seq2seq_model_architecture.svg?sanitize=true)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "WT8maZlD0v-A"
   },
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "JxHP00h6FgL0"
   },
   "source": [
    "The below hyperparameters were found through some trial and error.\n",
    "\n",
    "**This should take approximately ~ 35 minutes to train.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 598
    },
    "colab_type": "code",
    "collapsed": true,
    "id": "HgUemXCbPZrk",
    "outputId": "de5e3c29-b174-478c-a7f3-614d253dc747"
   },
   "outputs": [],
   "source": [
    "batch_size = 1024\n",
    "epochs = 16\n",
    "\n",
    "seq2seq_model.compile(optimizer=optimizers.Nadam(lr=0.00005), loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
    "history = seq2seq_model.fit([encoder_input_data, decoder_input_data],\n",
    "                            np.expand_dims(decoder_target_data, -1),\n",
    "                            batch_size=batch_size,\n",
    "                            epochs=epochs,\n",
    "                            validation_split=0.1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Pau6RSHNTxIX"
   },
   "source": [
    "Recommendation of keeping track of different experiments:\n",
    "\n",
    "http://wandb.com\n",
    "\n",
    "(We will not be covering this in the tutorial)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "1lbdGPHE0v-B"
   },
   "source": [
    "## Extract encoder and decoder models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "xcl66yGRF11e"
   },
   "source": [
    "To prepare the model for inference (to make predictions), we have to re-assemble it (with its trained weights intact) such that the decoder uses the last prediction as input rather than being fed the right answer for the previous time step, as illustrated below:\n",
    "\n",
    "![alt text](https://blog.keras.io/img/seq2seq/seq2seq-inference.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "collapsed": true,
    "id": "UAqk3fIwPZrn"
   },
   "outputs": [],
   "source": [
    "def extract_decoder_model(model):\n",
    "    latent_dim = model.get_layer('Encoder-Model').output_shape[-1]\n",
    "    decoder_inputs = model.get_layer('Decoder-Input').input\n",
    "    dec_emb = model.get_layer('Decoder-Word-Embedding')(decoder_inputs)\n",
    "    dec_bn = model.get_layer('Decoder-Batchnorm-1')(dec_emb)\n",
    "    gru_inference_state_input = Input(shape=(latent_dim,), name='hidden_state_input')\n",
    "    gru_out, gru_state_out = model.get_layer('Decoder-GRU')([dec_bn, gru_inference_state_input])\n",
    "    dec_bn2 = model.get_layer('Decoder-Batchnorm-2')(gru_out)\n",
    "    dense_out = model.get_layer('Final-Output-Dense')(dec_bn2)\n",
    "    decoder_model = Model([decoder_inputs, gru_inference_state_input], [dense_out, gru_state_out])\n",
    "    return decoder_model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "PU8JuOvfGKta"
   },
   "source": [
    "One side effect of training a sequence-to-sequence model in this way is that the encoder can be re-used as a general purpose feature extractor. We extract the encoder below for this purpose in a later exercise."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 391
    },
    "colab_type": "code",
    "collapsed": true,
    "id": "Xe0_KakGPZrp",
    "outputId": "cfbe231c-17b4-4e2c-ae06-02a3203f9504"
   },
   "outputs": [],
   "source": [
    "encoder_model = seq2seq_model.get_layer('Encoder-Model')\n",
    "for layer in encoder_model.layers:\n",
    "    layer.trainable = False\n",
    "\n",
    "decoder_model = extract_decoder_model(seq2seq_model)\n",
    "decoder_model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ZFdwR6UR0v-L"
   },
   "source": [
    "## Predict code descriptions using the trained sequence-to-sequence model\n",
    "\n",
    "You will see that the predicted descriptions are not perfect, but seem to be picking up on correlations between common code token sequences and natural language descriptions of that code.\n",
    "\n",
    "Feel free to run the below block of code as many times as you want. A new random sample from the test set will be drawn each time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 204
    },
    "colab_type": "code",
    "collapsed": true,
    "id": "JvK1eN9fPZrr",
    "outputId": "1b6c3eab-8204-4090-80f0-1bc818aec6a3"
   },
   "outputs": [],
   "source": [
    "i = random.randint(0, len(test_source_docs))\n",
    "\n",
    "max_len = target_proc.padding_maxlen\n",
    "raw_input_text = test_source_docs[i]\n",
    "\n",
    "raw_tokenized = source_proc.transform([raw_input_text])\n",
    "encoding = encoder_model.predict(raw_tokenized)\n",
    "original_encoding = encoding\n",
    "state_value = np.array(target_proc.token2id['_start_']).reshape(1, 1)\n",
    "\n",
    "decoded_sentence = []\n",
    "stop_condition = False\n",
    "while not stop_condition:\n",
    "    preds, st = decoder_model.predict([state_value, encoding])\n",
    "    pred_idx = np.argmax(preds[:, :, 2:]) + 2\n",
    "    pred_word_str = target_proc.id2token[pred_idx]\n",
    "\n",
    "    if pred_word_str == '_end_' or len(decoded_sentence) >= max_len:\n",
    "        stop_condition = True\n",
    "        break\n",
    "    decoded_sentence.append(pred_word_str)\n",
    "\n",
    "    # update the decoder for the next word\n",
    "    encoding = st\n",
    "    state_value = np.array(pred_idx).reshape(1, 1)\n",
    "\n",
    "print('sample code from test set:\\n------------------------\\n', raw_input_text)\n",
    "print('\\nground truth:\\n------------------------\\n', test_target_docs[i])\n",
    "print('\\npredicted description:\\n------------------------')\n",
    "print(' '.join(decoded_sentence))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "tf2bITb3Ny7H"
   },
   "source": [
    "## Generate Embeddings\n",
    "\n",
    "We need two embeddings\n",
    "\n",
    "1.  Embeddings for the code snippets, from the seq2seq encoder.\n",
    "\n",
    "2. Embeddings for the docstrings, from the language model.\n",
    "\n",
    "\n",
    "\n",
    "Embeddings for the code snippets:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "collapsed": true,
    "id": "qYPM3Tqf0v-P"
   },
   "outputs": [],
   "source": [
    "train_source_emb = encoder_model.predict(source_proc.transform(train_source_docs))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "U9M3Uvslgnp9"
   },
   "source": [
    "Embeddings for the target documents, which are natural language summaries (like docstrings):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "collapsed": true,
    "id": "geb3SOyk0v-Q"
   },
   "outputs": [],
   "source": [
    "train_target_vecs = proc.transform(train_target_docs)\n",
    "hidden_states = embedding_model.predict(train_target_vecs[:, 1:])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "0cFA5iSciFOt"
   },
   "source": [
    "Summarize the hidden states from the languager model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "collapsed": true,
    "id": "sFlbJj02iKTv"
   },
   "outputs": [],
   "source": [
    "mean_vecs = np.mean(hidden_states, axis=1)\n",
    "max_vecs = np.max(hidden_states, axis=1)\n",
    "sum_vecs = np.sum(hidden_states, axis=1)\n",
    "train_target_emb = sum_vecs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "zO9Cyj3qiP7F"
   },
   "source": [
    "Check the shapes of each embedding."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 51
    },
    "colab_type": "code",
    "collapsed": true,
    "id": "e_f5ptIL0v-R",
    "outputId": "ac212a81-ce57-4bb5-a1da-fcdbafd94699"
   },
   "outputs": [],
   "source": [
    "print('source embedding shape on training set: ', train_source_emb.shape)\n",
    "print('target embedding shape on training set: ', train_target_emb.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "8xOKEWFj0v-S"
   },
   "source": [
    "# Construct a Joint Vector Space (Semantic Code Search)\n",
    "\n",
    "Right now we have a way of representing:\n",
    "- a blob of code as a vector using the encoder of the sequence-to-sequence model, and \n",
    "- the code descriptions as a vector using the language model.\n",
    "\n",
    "However, these two vector spaces are not related to eachother. It can be useful to project the vectors for code and descriptions into the same space so that we can search code with natural language. There are many ways of accomplishing this task, however we will demonstrate a technique inspired from [this paper](https://static.googleusercontent.com/media/research.google.com/en//pubs/archive/41473.pdf), where we use regression to \"pull\" these vectors into the same space.  This idea is further illustrated below:\n",
    "\n",
    "![alt text](https://cdn-images-1.medium.com/max/1280/1*zhLXNHK8ILaYV8tT-jDlOQ.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "taGJtaGIZWCg"
   },
   "source": [
    "### Review of the high-level process:  How do we build a joint vector space?  \n",
    "\n",
    "Surprise! You are almost there!  We have already completed steps 1 - 3 as illustrated below.   \n",
    "\n",
    "![alt text](https://raw.githubusercontent.com/hohsiangwu/kdd-2018-hands-on-tutorials/master/images/joint_space_diagram.svg?sanitize=true)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "kEuoPiJnebrm"
   },
   "source": [
    "Most of the pieces for this step come from prior steps in this tutorial. In this step, we will fine-tune the seq2seq model  to predict docstring embeddings instead of docstrings. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 204
    },
    "colab_type": "code",
    "collapsed": true,
    "id": "-noqpPJY0v-T",
    "outputId": "83271f7b-3f41-49d4-f684-2753b897dee4"
   },
   "outputs": [],
   "source": [
    "inp = Input(shape=(train_source_emb.shape[1],))\n",
    "x = Dense(train_target_emb.shape[1], use_bias=False)(inp)\n",
    "# x = BatchNormalization()(x)\n",
    "# x = Dense(512)(x)\n",
    "modal_model = Model([inp], x)\n",
    "modal_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 394
    },
    "colab_type": "code",
    "collapsed": true,
    "id": "7zALzqWO0v-U",
    "outputId": "16491e42-d7a0-4935-a963-e05c0eaa41bb"
   },
   "outputs": [],
   "source": [
    "modal_model.compile(optimizer=optimizers.Nadam(lr=0.002), loss='cosine_proximity', metrics=['accuracy'])\n",
    "\n",
    "batch_size = 1024\n",
    "epochs = 10\n",
    "history = modal_model.fit([train_source_emb], train_target_emb,\n",
    "                          batch_size=batch_size, epochs=epochs, validation_split=0.1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Re8nxdPs0v-Y"
   },
   "source": [
    "## Application - Semantic Search\n",
    "\n",
    "### Use test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "collapsed": true,
    "id": "ZX0AIo_B0v-Z"
   },
   "outputs": [],
   "source": [
    "test_source_emb = encoder_model.predict(source_proc.transform(test_source_docs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "collapsed": true,
    "id": "2BaHfbsl0v-a"
   },
   "outputs": [],
   "source": [
    "test_target_vecs = proc.transform(test_target_docs)\n",
    "hidden_states = embedding_model.predict(test_target_vecs[:, 1:])\n",
    "mean_vecs = np.mean(hidden_states, axis=1)\n",
    "max_vecs = np.max(hidden_states, axis=1)\n",
    "sum_vecs = np.sum(hidden_states, axis=1)\n",
    "test_target_emb = sum_vecs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 51
    },
    "colab_type": "code",
    "collapsed": true,
    "id": "QgKykGVh0v-b",
    "outputId": "880e9dbc-6416-4034-aea7-0f97e86fc7eb"
   },
   "outputs": [],
   "source": [
    "print(test_source_emb.shape)\n",
    "print(test_target_emb.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "eg7JX1Vu0v-d"
   },
   "source": [
    "### Build vector indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "collapsed": true,
    "id": "D7HKpc3j0v-d",
    "outputId": "35e2e941-e869-4afd-b390-3da918252a1e"
   },
   "outputs": [],
   "source": [
    "dimension = hidden_states.shape[-1]\n",
    "index = AnnoyIndex(dimension)\n",
    "for i, v in enumerate(test_target_emb):\n",
    "    index.add_item(i, v)\n",
    "index.build(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "C-fpIqvQ0v-f"
   },
   "source": [
    "### Search nearest neighbors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 204
    },
    "colab_type": "code",
    "collapsed": true,
    "id": "zPlMXs500v-g",
    "outputId": "d68da176-3502-486b-8710-9adde0d58668"
   },
   "outputs": [],
   "source": [
    "i = random.randint(0, len(test_source_docs))\n",
    "input_sequence = test_source_docs[i]\n",
    "print(input_sequence)\n",
    "\n",
    "vec = np.expand_dims(test_source_emb[i], 0)\n",
    "out_vec = modal_model.predict(vec)\n",
    "ids, _ = index.get_nns_by_vector(out_vec.T, 10, include_distances=True)\n",
    "[test_target_docs[i] for i in ids]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "collapsed": true,
    "id": "U-u1WX4EQa83"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "name": "Feature Extraction and Summarization with Sequence to Sequence Learning.ipynb",
   "provenance": [],
   "toc_visible": true,
   "version": "0.3.2"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
